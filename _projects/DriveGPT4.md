---
layout: project_page
category: work

projectname: DriveGPT4
title: Interpretable End-to-end Autonomous Driving via Large Language Model
author1: Zhenhua Xu
author1link: https://tonyxuqaq.github.io/
author2: Yujia Zhang
affiliations:
    The University of Hong Kong 
affiliations2:
    Zhejiang University
paper: ../../assets/pdf/DriveGPT4.pdf
arxiv: https://arxiv.org/abs/2310.01412
video: https://youtu.be/CPoskBNjHlk
code: https://arxiv.org/abs/2310.01412
data: https://arxiv.org/abs/2310.01412
---


<!-- > Note: This is an example of a Jekyll-based project website template: [Github link](https://github.com/shunzh/project_website).\
> The following content is generated by ChatGPT. The figure is manually added. -->
<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Demo Video</h2>
        <p align="center"><iframe width="800" height="450" src="https://www.youtube.com/embed/CPoskBNjHlk" frameborder="0" allowfullscreen></iframe></p>
    </div>
</div>

---
<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Abstract</h2>
        <div class="content has-text-justified">
Multimodal large language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion.
These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy.  DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V. The code and dataset will be publicly available.
        </div>
    </div>
</div>

---





<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Demo Figures</h2>
    </div>
</div>
We provide some demos for DriveGPT4 generated conversations.

**BDD-X testing set:**
<p align="center">
  <img style="width: 49%" src="img/example_1-1.png" />
  <img style="width: 49%" src="img/example_2-1.png" />
</p>
**NuScenes validation set:**
<p align="center">
  <img style="width: 49%" src="img/example_3-1.png" />
  <img style="width: 49%" src="img/example_4-1.png" />
</p>
**Video games:**
<p align="center">
  <img style="width: 49%" src="img/example_gta-1.png" />
</p>
**Additional results on the BDD-X testing set:**
<p align="center">
  <img style="width: 49%" src="img/ex1-1.png" />
  <img style="width: 49%" src="img/ex2-1.png" />
</p>
<p align="center">
  <img style="width: 49%" src="img/ex3-1.png" />
  <img style="width: 49%" src="img/ex4-1.png" />
</p>
<p align="center">
  <img style="width: 49%" src="img/ex5-1.png" />
  <img style="width: 49%" src="img/ex6-1.png" />
</p>
<p align="center">
  <img style="width: 49%" src="img/ex7-1.png" />
  <img style="width: 49%" src="img/ex8-1.png" />
</p>

---
## Contact
For any questions, please send email to zhxuv at hku dot hkã€‚

---
## Citation
```
@article{xu2023drivegpt4,
  title={DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model},
  author={Xu, Zhenhua and Zhang, Yujia and Xie, Enze and Zhao, Zhen and Guo, Yong and Wong, Kenneth KY and Li, Zhenguo and Zhao, Hengshuang},
  journal={arXiv preprint arXiv:2310.01412},
  year={2023}
}
```